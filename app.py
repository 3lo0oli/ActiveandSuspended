import streamlit as st
import httpx
import time
from concurrent.futures import ThreadPoolExecutor

st.set_page_config(page_title="Social Media Status Checker", page_icon="ğŸ”", layout="wide")

st.title("ğŸ” ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ø³ÙˆØ´ÙŠØ§Ù„ Ù…ÙŠØ¯ÙŠØ§")
st.markdown("""
<div style='background-color:#e6f2ff;padding:15px;border-radius:10px;margin-bottom:20px'>
Ø§ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨Ø§Øª Twitter, Facebook, Instagram, TikTok, YouTube - Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
</div>
""", unsafe_allow_html=True)

# ==================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª RapidAPI ====================
RAPIDAPI_KEY = st.sidebar.text_input("ğŸ”‘ RapidAPI Key", type="password", help="Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† https://rapidapi.com")

# ==================== Ø¯ÙˆØ§Ù„ Ø§Ù„ÙƒØ´Ù ====================

def detect_platform(url):
    """ÙƒØ´Ù Ø§Ù„Ù…Ù†ØµØ© Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    url = url.lower()
    if 'twitter.com' in url or 'x.com' in url:
        return 'twitter'
    elif 'facebook.com' in url or 'fb.com' in url:
        return 'facebook'
    elif 'instagram.com' in url:
        return 'instagram'
    elif 'tiktok.com' in url:
        return 'tiktok'
    elif 'youtube.com' in url or 'youtu.be' in url:
        return 'youtube'
    return None

def extract_username(url, platform):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    import re
    url = url.strip()
    
    if platform == 'twitter':
        match = re.search(r'(?:twitter\.com|x\.com)/([^/?]+)', url)
        return match.group(1) if match else url.replace('@', '')
    elif platform == 'facebook':
        match = re.search(r'facebook\.com/([^/?]+)', url)
        return match.group(1) if match else url
    elif platform == 'instagram':
        match = re.search(r'instagram\.com/([^/?]+)', url)
        return match.group(1) if match else url
    elif platform == 'tiktok':
        match = re.search(r'tiktok\.com/@?([^/?]+)', url)
        return match.group(1) if match else url.replace('@', '')
    elif platform == 'youtube':
        if '/channel/' in url:
            return url.split('/channel/')[-1].split('/')[0]
        elif '/@' in url:
            return url.split('/@')[-1].split('/')[0]
        elif '/c/' in url:
            return url.split('/c/')[-1].split('/')[0]
        else:
            match = re.search(r'youtube\.com/([^/?]+)', url)
            return match.group(1) if match else url
    
    return url

# ==================== Ø¯ÙˆØ§Ù„ Ø§Ù„ÙØ­Øµ Ø¨Ù€ RapidAPI ====================

def check_twitter_api(username, api_key):
    """ÙØ­Øµ Twitter Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RapidAPI"""
    if not api_key:
        return check_twitter_scrape(username)
    
    url = "https://twitter-api45.p.rapidapi.com/screenname.php"
    headers = {
        "X-RapidAPI-Key": api_key,
        "X-RapidAPI-Host": "twitter-api45.p.rapidapi.com"
    }
    params = {"screenname": username}
    
    try:
        response = httpx.get(url, headers=headers, params=params, timeout=15)
        data = response.json()
        
        if 'error' in data:
            return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", f"https://twitter.com/{username}"
        elif data.get('suspended') == True:
            return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", f"https://twitter.com/{username}"
        elif data.get('screen_name'):
            return "âœ… Ù†Ø´Ø·", f"https://twitter.com/{username}"
        else:
            return "âš ï¸ ØºÙŠØ± ÙˆØ§Ø¶Ø­", f"https://twitter.com/{username}"
    except:
        return check_twitter_scrape(username)

def check_instagram_api(username, api_key):
    """ÙØ­Øµ Instagram Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RapidAPI"""
    if not api_key:
        return check_instagram_scrape(username)
    
    url = "https://instagram-scraper-api2.p.rapidapi.com/v1/info"
    headers = {
        "X-RapidAPI-Key": api_key,
        "X-RapidAPI-Host": "instagram-scraper-api2.p.rapidapi.com"
    }
    params = {"username_or_id_or_url": username}
    
    try:
        response = httpx.get(url, headers=headers, params=params, timeout=15)
        data = response.json()
        
        if data.get('status') == 'ok' and data.get('data'):
            user_data = data['data']
            if user_data.get('is_private') is not None:
                return "âœ… Ù†Ø´Ø·", f"https://instagram.com/{username}"
            else:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", f"https://instagram.com/{username}"
        else:
            return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", f"https://instagram.com/{username}"
    except:
        return check_instagram_scrape(username)

def check_tiktok_api(username, api_key):
    """ÙØ­Øµ TikTok Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RapidAPI"""
    if not api_key:
        return check_tiktok_scrape(username)
    
    url = "https://tiktok-scraper7.p.rapidapi.com/user/info"
    headers = {
        "X-RapidAPI-Key": api_key,
        "X-RapidAPI-Host": "tiktok-scraper7.p.rapidapi.com"
    }
    params = {"unique_id": username}
    
    try:
        response = httpx.get(url, headers=headers, params=params, timeout=15)
        data = response.json()
        
        if data.get('data') and data['data'].get('user'):
            return "âœ… Ù†Ø´Ø·", f"https://tiktok.com/@{username}"
        else:
            return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", f"https://tiktok.com/@{username}"
    except:
        return check_tiktok_scrape(username)

def check_youtube_api(username, api_key):
    """ÙØ­Øµ YouTube Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RapidAPI"""
    if not api_key:
        return check_youtube_scrape(username)
    
    # ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… YouTube Data API v3
    return check_youtube_scrape(username)

# ==================== Ø¯ÙˆØ§Ù„ Scraping Ø§Ù„Ù…Ø­Ø³Ù†Ø© (Fallback) ====================

def check_twitter_scrape(username):
    """ÙØ­Øµ Twitter Ø¹Ù† Ø·Ø±ÙŠÙ‚ Scraping Ù…Ø­Ø³Ù‘Ù†"""
    urls_to_try = [
        f"https://x.com/{username}",
        f"https://twitter.com/{username}",
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
    }
    
    for url in urls_to_try:
        try:
            with httpx.Client(timeout=20, follow_redirects=True) as client:
                response = client.get(url, headers=headers)
                
                # ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
                if response.status_code == 404:
                    continue
                
                # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©
                if "suspended" in response.text.lower():
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
                elif "doesn't exist" in response.text.lower() or "not found" in response.text.lower():
                    continue
                elif response.status_code == 200:
                    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø­Ø³Ø§Ø¨
                    if '"followers_count"' in response.text or '"following_count"' in response.text:
                        return "âœ… Ù†Ø´Ø·", url
                    elif len(response.text) > 50000:  # ØµÙØ­Ø© Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø¹Ø§Ø¯ÙŠØ©
                        return "âœ… Ù†Ø´Ø·", url
                    
        except Exception as e:
            continue
    
    return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", urls_to_try[0]

def check_facebook_scrape(username):
    """ÙØ­Øµ Facebook Ù…Ø­Ø³Ù‘Ù†"""
    url = f"https://www.facebook.com/{username}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept-Language": "en-US,en;q=0.9"
    }
    
    try:
        with httpx.Client(timeout=20, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            
            if response.status_code == 404:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            text = response.text.lower()
            
            # Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø­Ø°ÙˆÙ/Ø§Ù„Ù…Ø¹Ù„Ù‚
            if "content isn't available" in text or "page isn't available" in text:
                return "ğŸš« Ù…Ø¹Ù„Ù‚ Ø£Ùˆ Ù…Ø­Ø°ÙˆÙ", url
            elif "you must log in" in text or response.status_code == 200:
                return "âœ… Ù†Ø´Ø·", url
            
            return "âš ï¸ ØºÙŠØ± ÙˆØ§Ø¶Ø­", url
    except:
        return "â“ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„", url

def check_instagram_scrape(username):
    """ÙØ­Øµ Instagram Ù…Ø­Ø³Ù‘Ù†"""
    url = f"https://www.instagram.com/{username}/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "*/*",
        "Accept-Language": "en-US,en;q=0.9"
    }
    
    try:
        with httpx.Client(timeout=20, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            
            if response.status_code == 404:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            text = response.text.lower()
            
            if "page not found" in text or "sorry, this page" in text:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            elif '"is_private"' in text or '"edge_followed_by"' in text:
                return "âœ… Ù†Ø´Ø·", url
            elif response.status_code == 200 and len(response.text) > 10000:
                return "âœ… Ù†Ø´Ø·", url
            
            return "âš ï¸ ØºÙŠØ± ÙˆØ§Ø¶Ø­", url
    except:
        return "â“ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„", url

def check_tiktok_scrape(username):
    """ÙØ­Øµ TikTok Ù…Ø­Ø³Ù‘Ù†"""
    url = f"https://www.tiktok.com/@{username}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        with httpx.Client(timeout=20, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            
            if response.status_code == 404:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            text = response.text.lower()
            
            if "couldn't find" in text or "user not found" in text:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            elif '"followerCount"' in text or '"videoCount"' in text:
                return "âœ… Ù†Ø´Ø·", url
            elif response.status_code == 200:
                return "âœ… Ù†Ø´Ø·", url
            
            return "âš ï¸ ØºÙŠØ± ÙˆØ§Ø¶Ø­", url
    except:
        return "â“ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„", url

def check_youtube_scrape(username):
    """ÙØ­Øµ YouTube Ù…Ø­Ø³Ù‘Ù†"""
    urls = [
        f"https://www.youtube.com/@{username}",
        f"https://www.youtube.com/c/{username}",
        f"https://www.youtube.com/user/{username}"
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    for url in urls:
        try:
            with httpx.Client(timeout=20, follow_redirects=True) as client:
                response = client.get(url, headers=headers)
                
                if response.status_code == 404:
                    continue
                
                text = response.text.lower()
                
                if '"subscribercount"' in text or '"videoscount"' in text:
                    return "âœ… Ù†Ø´Ø·", url
                elif response.status_code == 200 and len(response.text) > 50000:
                    return "âœ… Ù†Ø´Ø·", url
        except:
            continue
    
    return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", urls[0]

# ==================== Ø¯Ø§Ù„Ø© Ø§Ù„ÙØ­Øµ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ====================

def check_account(url, api_key):
    """ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØµØ©"""
    platform = detect_platform(url)
    
    if not platform:
        return url, "â“ Ù…Ù†ØµØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©", url, "unknown"
    
    username = extract_username(url, platform)
    
    if not username:
        return url, "â“ Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ­ÙŠØ­", url, platform
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙØ­Øµ (API Ø£Ùˆ Scraping)
    if api_key and platform in ['twitter', 'instagram', 'tiktok']:
        checkers = {
            'twitter': lambda: check_twitter_api(username, api_key),
            'facebook': lambda: check_facebook_scrape(username),
            'instagram': lambda: check_instagram_api(username, api_key),
            'tiktok': lambda: check_tiktok_api(username, api_key),
            'youtube': lambda: check_youtube_api(username, api_key)
        }
    else:
        checkers = {
            'twitter': lambda: check_twitter_scrape(username),
            'facebook': lambda: check_facebook_scrape(username),
            'instagram': lambda: check_instagram_scrape(username),
            'tiktok': lambda: check_tiktok_scrape(username),
            'youtube': lambda: check_youtube_scrape(username)
        }
    
    status, final_url = checkers[platform]()
    
    return url, status, final_url, platform

# ==================== Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ====================

platform_icons = {
    'twitter': 'ğŸ¦',
    'facebook': 'ğŸ“˜',
    'instagram': 'ğŸ“¸',
    'tiktok': 'ğŸµ',
    'youtube': 'ğŸ“º',
    'unknown': 'â“'
}

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ù€ Sidebar
with st.sidebar:
    st.markdown("### â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª")
    if RAPIDAPI_KEY:
        st.success("âœ… API Ù…ÙØ¹Ù‘Ù„ - Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©")
    else:
        st.warning("âš ï¸ Ø¨Ø¯ÙˆÙ† API - Ø¯Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©")
    
    st.markdown("""
    **Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰:**
    1. Ø³Ø¬Ù„ ÙÙŠ [RapidAPI](https://rapidapi.com)
    2. Ø§Ø´ØªØ±Ùƒ ÙÙŠ APIs Ø§Ù„ØªØ§Ù„ÙŠØ©:
       - Twitter API45
       - Instagram Scraper
       - TikTok Scraper
    3. Ø¶Ø¹ API Key Ù‡Ù†Ø§ â˜ï¸
    """)

st.subheader("ğŸ“ Ø£Ø¯Ø®Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (Ø­ØªÙ‰ 10 Ø±ÙˆØ§Ø¨Ø·)")

urls_input = st.text_area(
    "Ø¶Ø¹ ÙƒÙ„ Ø±Ø§Ø¨Ø· ÙÙŠ Ø³Ø·Ø± Ù…Ù†ÙØµÙ„:",
    height=200,
    placeholder="https://twitter.com/username\nhttps://facebook.com/pagename\nhttps://instagram.com/username\n..."
)

col1, col2 = st.columns([1, 1])
with col1:
    check_button = st.button("ğŸ” ÙØ­Øµ Ø§Ù„ÙƒÙ„", type="primary", use_container_width=True)
with col2:
    clear_button = st.button("ğŸ—‘ï¸ Ù…Ø³Ø­", use_container_width=True)

if clear_button:
    st.rerun()

if check_button and urls_input.strip():
    urls = [url.strip() for url in urls_input.strip().split('\n') if url.strip()]
    
    if len(urls) > 10:
        st.warning("âš ï¸ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 10 Ø±ÙˆØ§Ø¨Ø·. Ø³ÙŠØªÙ… ÙØ­Øµ Ø£ÙˆÙ„ 10 ÙÙ‚Ø·.")
        urls = urls[:10]
    
    st.markdown("---")
    st.subheader(f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(urls)} Ø­Ø³Ø§Ø¨)")
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    results = []
    
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(check_account, url, RAPIDAPI_KEY) for url in urls]
        
        for i, future in enumerate(futures):
            result = future.result()
            results.append(result)
            
            progress = (i + 1) / len(urls)
            progress_bar.progress(progress)
            status_text.text(f"Ø¬Ø§Ø±Ù Ø§Ù„ÙØ­Øµ... {i+1}/{len(urls)}")
    
    progress_bar.empty()
    status_text.empty()
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    for original_url, status, final_url, platform in results:
        icon = platform_icons.get(platform, 'â“')
        
        col1, col2, col3 = st.columns([2, 3, 1])
        
        with col1:
            st.markdown(f"**{icon} {platform.upper()}**")
        
        with col2:
            if status.startswith("âœ…"):
                st.success(status)
            elif status.startswith("ğŸš«") or status.startswith("âŒ"):
                st.error(status)
            elif status.startswith("âš ï¸"):
                st.warning(status)
            else:
                st.info(status)
        
        with col3:
            st.markdown(f"[ğŸ”— Ø²ÙŠØ§Ø±Ø©]({final_url})")
        
        st.markdown("---")
    
    # Ù…Ù„Ø®Øµ
    active = sum(1 for _, status, _, _ in results if "âœ…" in status)
    suspended = sum(1 for _, status, _, _ in results if "ğŸš«" in status or "âŒ" in status)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("âœ… Ù†Ø´Ø·Ø©", active)
    with col2:
        st.metric("ğŸš« Ù…Ø¹Ù„Ù‚Ø©/Ù…Ø­Ø°ÙˆÙØ©", suspended)
    with col3:
        st.metric("ğŸ“Š Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹", len(results))

elif check_button:
    st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±ÙˆØ§Ø¨Ø· Ø£ÙˆÙ„Ø§Ù‹.")

st.markdown("---")
st.caption("ğŸ”§ ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit | Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù€ RapidAPI & Web Scraping")
