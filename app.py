import streamlit as st
import httpx
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urlparse

# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.set_page_config(
    page_title="ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Reddit", 
    page_icon="ğŸ”", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ØªØµÙ…ÙŠÙ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
st.title("ğŸ” ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Reddit")
st.markdown("""
<div style='text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px; margin-bottom: 30px;'>
    <h3 style='color: #FF4500;'>Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø³Ø§Ø¨Ø§Øª Reddit</h3>
    <p>ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø£ÙŠ Ø­Ø³Ø§Ø¨ Reddit (Ù†Ø´Ø·/Ù…ÙˆÙ‚ÙˆÙ/ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯)</p>
</div>
""", unsafe_allow_html=True)

# Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
def clean_username(input_text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„"""
    input_text = input_text.strip()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    if "reddit.com" in input_text:
        match = re.search(r'/(?:u|user)/([^/?]+)', input_text)
        if match:
            return match.group(1)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    input_text = re.sub(r'^(u/|user/|@|/)', '', input_text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©
    username = re.sub(r'[^a-zA-Z0-9_-]', '', input_text)
    
    return username

def build_reddit_url(username):
    """Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Reddit Ø§Ù„ØµØ­ÙŠØ­"""
    return f"https://www.reddit.com/user/{username}"

def check_reddit_status(username):
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…Ø¹ ÙƒØ´Ù Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø£ÙƒÙˆØ§Ø¯ HTTP"""
    if not username or len(username) < 3:
        return "âŒ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± ØµØ§Ù„Ø­", None
    
    url = build_reddit_url(username)
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Cache-Control": "no-cache"
    }
    
    try:
        with httpx.Client(timeout=25, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            
            # Ù„Ø§ Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø© - Ù†Ø­Ù„Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¨Ø§Ø´Ø±Ø©
            if response.status_code == 404:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            # Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† 403 Ø£Ùˆ Ø£ÙŠ ÙƒÙˆØ¯ Ø¢Ø®Ø±ØŒ Ù†Ø­Ù„Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø­Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù…Ù† HTML
            full_text = soup.get_text(separator=' ', strip=True).lower()
            
            # ============ 1. ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙˆÙ‚ÙˆÙØ© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ============
            
            # Ø£) ÙØ­Øµ shreddit-forbidden Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¯Ù‚ÙŠÙ‚
            forbidden_div = soup.find('div', {'id': 'shreddit-forbidden'})
            if forbidden_div:
                forbidden_text = forbidden_div.get_text().lower()
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù
                if ("suspended" in forbidden_text and "account" in forbidden_text) or \
                   ("this account has been suspended" in forbidden_text):
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
                
                # ÙØ­Øµ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø­Ø¯Ø¯
                title_element = forbidden_div.find('h1', {'id': 'shreddit-forbidden-title'})
                if title_element:
                    title_text = title_element.get_text().lower()
                    if "suspended" in title_text:
                        return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
            
            # Ø¨) ÙØ­Øµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹
            suspended_exact_phrases = [
                "this account has been suspended",
                "account has been suspended", 
                "user has been suspended",
                "account is suspended",
                "permanently suspended",
                "temporarily suspended"
            ]
            
            for phrase in suspended_exact_phrases:
                if phrase in full_text:
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
            
            # ============ 2. ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© ============
            deleted_exact_phrases = [
                "this user has deleted their account",
                "user deleted their account",
                "account has been deleted",
                "deleted their account"
            ]
            
            for phrase in deleted_exact_phrases:
                if phrase in full_text:
                    return "ğŸ—‘ï¸ Ù…Ø­Ø°ÙˆÙ", url
            
            # ============ 3. ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø© (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù†Ø´Ø§Ø·) ============
            
            # Ø£) Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ Ø§Ù„Ù†Ø´Ø·
            active_profile_indicators = [
                # Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ
                soup.find('div', {'data-testid': 'user-profile'}),
                soup.find('div', {'data-testid': 'profile-hover-card'}),
                soup.find('section', {'aria-label': lambda x: x and 'profile' in x.lower()}),
                soup.find('main', {'role': 'main'}),
                
                # Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                soup.select('div[data-testid*="post"]'),
                soup.select('div[data-testid*="comment"]'),
                soup.select('article'),
                
                # Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙ†Ù‚Ù„
                soup.find('nav'),
                soup.select('a[href*="posts"]'),
                soup.select('a[href*="comments"]'),
                soup.select('a[href*="overview"]')
            ]
            
            has_profile_elements = any(indicator for indicator in active_profile_indicators if indicator)
            
            # Ø¨) ÙØ­Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø´Ø·
            active_keywords = [
                "post karma", "comment karma", "awardee karma",
                "cake day", "joined", "reddit premium",
                "trophy case", "overview", "posts", "comments",
                "about", "karma", "achievements", "badges",
                "submitted", "gilded", "saved"
            ]
            
            active_keyword_matches = sum(1 for keyword in active_keywords if keyword in full_text)
            
            # Ø¬) ÙØ­Øµ Ø¹Ù†Ø§ØµØ± ÙˆØ§Ø¬Ù‡Ø© Reddit Ø§Ù„Ù†Ø´Ø·Ø©
            ui_elements = [
                soup.find('button'),
                soup.find('input'),
                soup.select('[class*="vote"]'),
                soup.select('[class*="karma"]'),
                soup.select('[data-testid]')
            ]
            
            has_ui_elements = any(element for element in ui_elements if element)
            
            # Ø¯) ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            user_content_indicators = [
                "redditor for", "joined reddit", "reddit birthday",
                "post history", "comment history", "user since"
            ]
            
            has_user_content = any(indicator in full_text for indicator in user_content_indicators)
            
            # ============ 4. Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø­Ø³Ù† (Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù†Ø´Ø§Ø·) ============
            
            # Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯Øª Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ Ø£Ùˆ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ÙƒØ«ÙŠØ±Ø©
            if has_profile_elements or active_keyword_matches >= 2:
                return "âœ… Ù†Ø´Ø·", url
            
            # Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯Øª Ø¹Ù†Ø§ØµØ± ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… + Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            elif has_ui_elements and has_user_content:
                return "âœ… Ù†Ø´Ø·", url
            
            # Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯Øª ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ù†Ø´Ø§Ø·
            elif active_keyword_matches >= 1:
                return "âœ… Ù†Ø´Ø·", url
            
            # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ Reddit Ø¹Ø§Ù… (Ø­ØªÙ‰ Ù„Ùˆ Ù…Ø­Ø¯ÙˆØ¯)
            elif "reddit" in full_text and len(full_text) > 100:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø±Ø³Ø§Ø¦Ù„ Ø®Ø·Ø£ ÙˆØ§Ø¶Ø­Ø©
                clear_error_indicators = [
                    "page not found", "user not found", "doesn't exist",
                    "no longer available", "been removed"
                ]
                has_clear_errors = any(error in full_text for error in clear_error_indicators)
                
                if not has_clear_errors:
                    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
                    if any(word in full_text for word in ["user", "profile", "redditor", "account"]):
                        return "âœ… Ù†Ø´Ø·", url
                    else:
                        return "â“ Ø­Ø§Ù„Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©", url
                else:
                    return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ø£ÙŠ Ø¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ø¶Ø­Ø©
            else:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
                    
    except httpx.TimeoutException:
        return "â±ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„", url
    except httpx.ConnectError:
        return "ğŸŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª", url
    except Exception as e:
        return f"âš ï¸ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)[:50]}...", url

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
col1, col2 = st.columns([3, 1])

with col1:
    user_input = st.text_input(
        "Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨:",
        placeholder="Ù…Ø«Ø§Ù„: username Ø£Ùˆ u/username Ø£Ùˆ https://reddit.com/u/username",
        help="ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø£ÙŠ ØµÙŠØºØ©"
    )

with col2:
    st.markdown("<br>", unsafe_allow_html=True)  # Ù…Ø³Ø§Ø­Ø© ÙØ§Ø±ØºØ© Ù„Ù„Ù…Ø­Ø§Ø°Ø§Ø©
    check_button = st.button("ğŸ” ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨", type="primary")

if check_button and user_input.strip():
    username = clean_username(user_input)
    
    if not username:
        st.error("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… ØµØ§Ù„Ø­")
    else:
        # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙØ­Øµ
        st.info(f"ğŸ” Ø¬Ø§Ø±Ù ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨: **{username}**")
        
        # Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i in range(100):
            progress_bar.progress(i + 1)
            if i < 30:
                status_text.text("Ø¬Ø§Ø±Ù Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Reddit...")
            elif i < 70:
                status_text.text("Ø¬Ø§Ø±Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            else:
                status_text.text("Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")
            time.sleep(0.01)
        
        # ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨
        status, url = check_reddit_status(username)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar.empty()
        status_text.empty()
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¹ ØªØµÙ…ÙŠÙ… Ù…Ù…ÙŠØ²
        st.markdown("---")
        st.subheader("ğŸ“Š Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ:")
        
        if status.startswith("âœ…"):
            st.success(f"**{status}**")
            st.balloons()  # ØªØ£Ø«ÙŠØ± Ø¨ØµØ±ÙŠ Ù„Ù„Ù†Ø¬Ø§Ø­
            if url:
                st.markdown(f"ğŸ”— [Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø­Ø³Ø§Ø¨]({url})")
        elif status.startswith("ğŸš«"):
            st.error(f"**{status}**")
        elif status.startswith("âŒ") or status.startswith("ğŸ—‘ï¸"):
            st.warning(f"**{status}**")
        else:
            st.info(f"**{status}**")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        with st.expander("ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙØ­Øµ"):
            st.write(f"**Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {username}")
            if url:
                st.write(f"**Ø§Ù„Ø±Ø§Ø¨Ø·:** {url}")
            st.write(f"**ÙˆÙ‚Øª Ø§Ù„ÙØ­Øµ:** {time.strftime('%Y-%m-%d %H:%M:%S')}")

elif check_button:
    st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨ Ø£ÙˆÙ„Ø§Ù‹")



# ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>ğŸ› ï¸ ØªÙ… ØªØ·ÙˆÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ÙØ­Øµ Ø­Ø³Ø§Ø¨Ø§Øª Reddit Ø¨Ø³Ù‡ÙˆÙ„Ø© ÙˆØ³Ø±Ø¹Ø©</p>
    <p>ğŸ’» Ù…Ø·ÙˆØ± Ø¨ØªÙ‚Ù†ÙŠØ© Streamlit | ğŸ”’ Ø¢Ù…Ù† ÙˆØ³Ø±ÙŠØ¹</p>
</div>
""", unsafe_allow_html=True)
