import streamlit as st
import httpx
import re
from urllib.parse import urlparse
from bs4 import BeautifulSoup

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø³Ø§Ø¨Ø§Øª Reddit Ø¨Ø¯Ù‚Ø©",
    page_icon="ğŸ”",
    layout="centered"
)

# CSS Ù…Ø®ØµØµ
st.markdown("""
<style>
    .header {
        text-align: center;
        color: #FF4500;
        margin-bottom: 30px;
    }
    .result-box {
        padding: 20px;
        border-radius: 8px;
        margin: 20px 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    .active { border-left: 5px solid #4CAF50; background-color: #e8f5e9; }
    .suspended { border-left: 5px solid #f44336; background-color: #ffebee; }
    .not-found { border-left: 5px solid #FF9800; background-color: #fff3e0; }
    .unknown { border-left: 5px solid #9e9e9e; background-color: #f5f5f5; }
    .stButton>button {
        background-color: #FF4500;
        color: white;
        font-weight: bold;
        border: none;
        padding: 0.5rem 1rem;
        border-radius: 0.5rem;
        width: 100%;
    }
    .user-info {
        background: #f0f2f5;
        padding: 15px;
        border-radius: 8px;
        margin-top: 15px;
    }
</style>
""", unsafe_allow_html=True)

# Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.markdown("<h1 class='header'>ğŸ” Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø³Ø§Ø¨Ø§Øª Reddit Ø¨Ø¯Ù‚Ø©</h1>", unsafe_allow_html=True)

# Ø¯Ø§Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·
def extract_reddit_username(input_url):
    try:
        if not input_url:
            return None
            
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        input_url = input_url.strip().strip("/").replace("https://", "").replace("http://", "")
        
        if "reddit.com" not in input_url:
            return input_url.split("/")[0].replace("u/", "").replace("@", "")
        
        if "/user/" in input_url:
            return input_url.split("/user/")[-1].split("/")[0]
        elif "/u/" in input_url:
            return input_url.split("/u/")[-1].split("/")[0]
        
        return input_url.split("reddit.com/")[-1].split("/")[0]
    
    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {str(e)}")
        return None

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…Ø¹ ÙØ­Øµ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
def check_reddit_account(username):
    if not username:
        return "âŒ Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… ØµØ­ÙŠØ­", "unknown", None
    
    url = f"https://www.reddit.com/user/{username}/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9"
    }
    
    try:
        with httpx.Client(follow_redirects=True, timeout=15) as client:
            response = client.get(url, headers=headers)
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆÙ‚ÙˆÙ
            suspended_keywords = [
                "this account has been suspended",
                "account suspended",
                "suspended account",
                "content unavailable"
            ]
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
            not_found_keywords = [
                "page not found",
                "sorry, nobody on reddit goes by that name",
                "there's nobody on reddit by that name"
            ]
            
            # Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ØµÙØ­Ø©
            page_text = soup.get_text().lower()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆÙ‚ÙˆÙ
            if any(keyword in page_text for keyword in suspended_keywords):
                return "ğŸ”´ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…ÙˆÙ‚ÙˆÙ (Suspended)", "suspended", url
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
            if response.status_code == 404 or any(keyword in page_text for keyword in not_found_keywords):
                return "âŒ Ø§Ù„Ø­Ø³Ø§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ (404)", "not-found", url
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø´Ø·
            if response.status_code == 200:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø®Ø§ØµØ© Ø¨Ø­Ø³Ø§Ø¨ Ù†Ø´Ø·
                user_profile = soup.find("div", {"class": "profile-header"})
                user_posts = soup.find("div", {"id": "profile-posts"})
                
                if user_profile or user_posts:
                    return "ğŸŸ¢ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù†Ø´Ø· (Active)", "active", url
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©
            return "âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø© Ø¨Ø¯Ù‚Ø©", "unknown", url
    
    except httpx.TimeoutException:
        return "âš ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨", "unknown", url
    except Exception as e:
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}", "unknown", url

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
input_url = st.text_input(
    "Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø­Ø³Ø§Ø¨ Reddit Ø£Ùˆ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:",
    placeholder="Ù…Ø«Ø§Ù„: hedaa_7 Ø£Ùˆ https://www.reddit.com/user/hedaa_7/",
    key="user_input"
)

check_button = st.button("ØªØ­Ù‚Ù‚ Ø§Ù„Ø¢Ù†")

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
if check_button:
    if input_url:
        with st.spinner("Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¨Ø¯Ù‚Ø©..."):
            username = extract_reddit_username(input_url)
            
            if username:
                status, status_class, profile_url = check_reddit_account(username)
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                st.markdown(
                    f"""
                    <div class="result-box {status_class}">
                        <h3>{status}</h3>
                        <div class="user-info">
                            <p><strong>Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:</strong> {username}</p>
                            <p><strong>Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨:</strong> <a href="{profile_url}" target="_blank">{profile_url}</a></p>
                        </div>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
                
                # Ø¥Ø¶Ø§ÙØ© ØªÙØ³ÙŠØ± Ù„Ù„Ø­Ø§Ù„Ø©
                if status_class == "active":
                    st.success("ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø³Ø§Ø¨ ÙˆÙ‡Ùˆ Ù†Ø´Ø·. ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø±Ø§Ø¨Ø· Ø£Ø¹Ù„Ø§Ù‡ Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰.")
                elif status_class == "suspended":
                    st.error("Ù‡Ø°Ø§ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…ÙˆÙ‚ÙˆÙ Ù…Ù† Ù‚Ø¨Ù„ Ø¥Ø¯Ø§Ø±Ø© Reddit. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­ØªÙˆÙ‰.")
                elif status_class == "not-found":
                    st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ø³Ø§Ø¨ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³Ù…. ØªØ£ÙƒØ¯ Ù…Ù† ÙƒØªØ§Ø¨Ø© Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.")
                else:
                    st.info("Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¨Ø¯Ù‚Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù„ØªØ­Ù‚Ù‚ ÙŠØ¯ÙˆÙŠÙ‹Ø§.")
            else:
                st.error("âš ï¸ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… ØµØ­ÙŠØ­ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¯Ø®Ù„")
    else:
        st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨ Ø£Ùˆ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£ÙˆÙ„Ø§Ù‹")

# ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
st.markdown("---")
st.markdown("""
<p style="text-align: center; color: #666; font-size: 0.9rem;">
    Ø£Ø¯Ø§Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø³Ø§Ø¨Ø§Øª Reddit | ØªØ¹Ù…Ù„ Ø¨ÙØ­Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù„ØµÙØ­Ø§Øª
</p>
""", unsafe_allow_html=True)
