import streamlit as st
import httpx
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urlparse

# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.set_page_config(
    page_title="ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Reddit", 
    page_icon="ğŸ”", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ØªØµÙ…ÙŠÙ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
st.title("ğŸ” ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Reddit")
st.markdown("""
<div style='text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px; margin-bottom: 30px;'>
    <h3 style='color: #FF4500;'>Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø³Ø§Ø¨Ø§Øª Reddit</h3>
    <p>ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø£ÙŠ Ø­Ø³Ø§Ø¨ Reddit (Ù†Ø´Ø·/Ù…ÙˆÙ‚ÙˆÙ/ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯)</p>
</div>
""", unsafe_allow_html=True)

# Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
def clean_username(input_text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„"""
    input_text = input_text.strip()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    if "reddit.com" in input_text:
        match = re.search(r'/(?:u|user)/([^/?]+)', input_text)
        if match:
            return match.group(1)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    input_text = re.sub(r'^(u/|user/|@|/)', '', input_text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©
    username = re.sub(r'[^a-zA-Z0-9_-]', '', input_text)
    
    return username

def build_reddit_url(username):
    """Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Reddit Ø§Ù„ØµØ­ÙŠØ­"""
    return f"https://www.reddit.com/user/{username}"

def check_reddit_status(username):
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…Ø¹ ÙƒØ´Ù Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    if not username or len(username) < 3:
        return "âŒ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± ØµØ§Ù„Ø­", None
    
    url = build_reddit_url(username)
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }
    
    try:
        with httpx.Client(timeout=20, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø£ÙˆÙ„Ø§Ù‹
            if response.status_code == 404:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            elif response.status_code == 403:
                return "ğŸš« Ù…Ø­Ø¸ÙˆØ± Ø£Ùˆ Ù…ÙˆÙ‚ÙˆÙ", url
            elif response.status_code != 200:
                return f"âš ï¸ Ø®Ø·Ø£ HTTP: {response.status_code}", url
            
            # ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # 1. ÙØ­Øµ shreddit-forbidden Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ (Ø§Ù„Ø£Ù‡Ù…)
            forbidden_div = soup.find('div', {'id': 'shreddit-forbidden'})
            if forbidden_div:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø¯Ø§Ø®Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù†ØµØ±
                suspended_title = forbidden_div.find('h1', {'id': 'shreddit-forbidden-title'})
                if suspended_title and "suspended" in suspended_title.get_text().lower():
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
                
                # Ø£Ùˆ ÙØ­Øµ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ù…Ø­Ø¸ÙˆØ±
                forbidden_text = forbidden_div.get_text().lower()
                if "suspended" in forbidden_text:
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
                elif "not found" in forbidden_text or "doesn't exist" in forbidden_text:
                    return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            # 2. ÙØ­Øµ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø£Ø®Ø±Ù‰ Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù
            suspended_elements = [
                soup.find('div', {'id': re.compile(r'.*suspend.*', re.I)}),
                soup.find('div', {'class': re.compile(r'.*suspend.*', re.I)}),
                soup.find('h1', string=re.compile(r'.*suspended.*', re.I)),
                soup.find('p', string=re.compile(r'.*suspended.*', re.I))
            ]
            
            for element in suspended_elements:
                if element:
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
            
            # 3. ÙØ­Øµ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            full_text = soup.get_text(separator=' ', strip=True).lower()
            
            # Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆÙ‚ÙˆÙ
            suspended_patterns = [
                "this account has been suspended",
                "account has been suspended", 
                "user has been suspended",
                "suspended account",
                "account suspended",
                "permanently suspended",
                "temporarily suspended",
                "banned account",
                "account banned"
            ]
            
            for pattern in suspended_patterns:
                if pattern in full_text:
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
            
            # 4. ÙØ­Øµ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø­Ø°ÙˆÙ
            deleted_patterns = [
                "this user has deleted their account",
                "account has been deleted",
                "user deleted their account",
                "deleted account",
                "account deleted"
            ]
            
            for pattern in deleted_patterns:
                if pattern in full_text:
                    return "ğŸ—‘ï¸ Ù…Ø­Ø°ÙˆÙ", url
            
            # 5. ÙØ­Øµ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙŠ ØªØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø³Ø§Ø¨ Ù†Ø´Ø·
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ Ø§Ù„Ù†Ø´Ø·
            active_selectors = [
                'div[data-testid="user-profile"]',
                'div[data-testid="profile-hover-card"]',
                'section[aria-label*="profile"]',
                'div[class*="profile"]',
                'div[data-testid*="post"]',
                'div[data-testid*="comment"]'
            ]
            
            has_active_elements = False
            for selector in active_selectors:
                if soup.select(selector):
                    has_active_elements = True
                    break
            
            # 6. ÙØ­Øµ Ø§Ù„Ù†Øµ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø´Ø·
            active_keywords = [
                "post karma", "comment karma", "joined reddit",
                "cake day", "trophy case", "overview",
                "posts", "comments", "about", "karma:",
                "reddit premium", "achievements"
            ]
            
            has_active_text = any(keyword in full_text for keyword in active_keywords)
            
            # 7. Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            if has_active_elements or has_active_text:
                return "âœ… Ù†Ø´Ø·", url
            elif "reddit" in full_text and len(full_text) > 300:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
                if any(word in full_text for word in ["user", "profile", "redditor"]):
                    return "âœ… Ù†Ø´Ø·", url
                else:
                    return "â“ Ø­Ø§Ù„Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©", url
            else:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
                    
    except httpx.TimeoutException:
        return "â±ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„", url
    except httpx.ConnectError:
        return "ğŸŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª", url
    except Exception as e:
        return f"âš ï¸ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)[:50]}...", url

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
col1, col2 = st.columns([3, 1])

with col1:
    user_input = st.text_input(
        "Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨:",
        placeholder="Ù…Ø«Ø§Ù„: username Ø£Ùˆ u/username Ø£Ùˆ https://reddit.com/u/username",
        help="ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø£ÙŠ ØµÙŠØºØ©"
    )

with col2:
    st.markdown("<br>", unsafe_allow_html=True)  # Ù…Ø³Ø§Ø­Ø© ÙØ§Ø±ØºØ© Ù„Ù„Ù…Ø­Ø§Ø°Ø§Ø©
    check_button = st.button("ğŸ” ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨", type="primary")

if check_button and user_input.strip():
    username = clean_username(user_input)
    
    if not username:
        st.error("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… ØµØ§Ù„Ø­")
    else:
        # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙØ­Øµ
        st.info(f"ğŸ” Ø¬Ø§Ø±Ù ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨: **{username}**")
        
        # Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i in range(100):
            progress_bar.progress(i + 1)
            if i < 30:
                status_text.text("Ø¬Ø§Ø±Ù Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Reddit...")
            elif i < 70:
                status_text.text("Ø¬Ø§Ø±Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            else:
                status_text.text("Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")
            time.sleep(0.01)
        
        # ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨
        status, url = check_reddit_status(username)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar.empty()
        status_text.empty()
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¹ ØªØµÙ…ÙŠÙ… Ù…Ù…ÙŠØ²
        st.markdown("---")
        st.subheader("ğŸ“Š Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ:")
        
        if status.startswith("âœ…"):
            st.success(f"**{status}**")
            st.balloons()  # ØªØ£Ø«ÙŠØ± Ø¨ØµØ±ÙŠ Ù„Ù„Ù†Ø¬Ø§Ø­
            if url:
                st.markdown(f"ğŸ”— [Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø­Ø³Ø§Ø¨]({url})")
        elif status.startswith("ğŸš«"):
            st.error(f"**{status}**")
        elif status.startswith("âŒ") or status.startswith("ğŸ—‘ï¸"):
            st.warning(f"**{status}**")
        else:
            st.info(f"**{status}**")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        with st.expander("ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙØ­Øµ"):
            st.write(f"**Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {username}")
            if url:
                st.write(f"**Ø§Ù„Ø±Ø§Ø¨Ø·:** {url}")
            st.write(f"**ÙˆÙ‚Øª Ø§Ù„ÙØ­Øµ:** {time.strftime('%Y-%m-%d %H:%M:%S')}")

elif check_button:
    st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨ Ø£ÙˆÙ„Ø§Ù‹")



# ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>ğŸ› ï¸ ØªÙ… ØªØ·ÙˆÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ÙØ­Øµ Ø­Ø³Ø§Ø¨Ø§Øª Reddit Ø¨Ø³Ù‡ÙˆÙ„Ø© ÙˆØ³Ø±Ø¹Ø©</p>
    <p>ğŸ’» Ù…Ø·ÙˆØ± Ø¨ØªÙ‚Ù†ÙŠØ© Streamlit | ğŸ”’ Ø¢Ù…Ù† ÙˆØ³Ø±ÙŠØ¹</p>
</div>
""", unsafe_allow_html=True)
