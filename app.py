import streamlit as st
import httpx
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urlparse

# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.set_page_config(
    page_title="ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Reddit", 
    page_icon="ğŸ”", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ØªØµÙ…ÙŠÙ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
st.title("ğŸ” ÙØ­Øµ Ø­Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Reddit")
st.markdown("""
<div style='text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px; margin-bottom: 30px;'>
    <h3 style='color: #FF4500;'>Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø³Ø§Ø¨Ø§Øª Reddit</h3>
    <p>ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø£ÙŠ Ø­Ø³Ø§Ø¨ Reddit (Ù†Ø´Ø·/Ù…ÙˆÙ‚ÙˆÙ/ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯)</p>
</div>
""", unsafe_allow_html=True)

# Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
def clean_username(input_text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„"""
    input_text = input_text.strip()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    if "reddit.com" in input_text:
        match = re.search(r'/(?:u|user)/([^/?]+)', input_text)
        if match:
            return match.group(1)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    input_text = re.sub(r'^(u/|user/|@|/)', '', input_text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©
    username = re.sub(r'[^a-zA-Z0-9_-]', '', input_text)
    
    return username

def build_reddit_url(username):
    """Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Reddit Ø§Ù„ØµØ­ÙŠØ­"""
    return f"https://www.reddit.com/user/{username}"

def check_reddit_status(username):
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…Ø¹ ÙƒØ´Ù Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…ØªÙˆØ§Ø²Ù†"""
    if not username or len(username) < 3:
        return "âŒ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± ØµØ§Ù„Ø­", None
    
    url = build_reddit_url(username)
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1"
    }
    
    try:
        with httpx.Client(timeout=20, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            
            # ÙØ­Øµ 404 Ø¨Ø³ (Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¤ÙƒØ¯)
            if response.status_code == 404:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
            full_text = soup.get_text(separator=' ', strip=True).lower()
            
            # ============ 1. ÙØ­Øµ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø£ÙˆÙ„Ø§Ù‹ (ØµØ§Ø±Ù… Ø¬Ø¯Ø§Ù‹) ============
            
            # Ø£) ÙØ­Øµ shreddit-forbidden Ù…Ø¹ Ø§Ù„ØªØ£ÙƒØ¯ Ø§Ù„Ù…Ø·Ù„Ù‚
            forbidden_div = soup.find('div', {'id': 'shreddit-forbidden'})
            if forbidden_div:
                forbidden_text = forbidden_div.get_text().strip().lower()
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚
                if "this account has been suspended" in forbidden_text:
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
                
                # ÙØ­Øµ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¨Ø¯Ù‚Ø©
                title_h1 = forbidden_div.find('h1')
                if title_h1:
                    title_text = title_h1.get_text().strip().lower()
                    if "suspended" in title_text and "account" in title_text:
                        return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
            
            # Ø¨) ÙØ­Øµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù
            suspended_phrases = [
                "this account has been suspended",
                "account has been suspended",
                "user has been suspended",
                "permanently suspended",
                "temporarily suspended"
            ]
            
            for phrase in suspended_phrases:
                if phrase in full_text:
                    return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
            
            # Ø¬) ÙØ­Øµ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ØµØµØ© Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù
            if soup.find('div', {'class': lambda x: x and 'suspended' in x.lower()}):
                return "ğŸš« Ù…ÙˆÙ‚ÙˆÙ", url
            
            # ============ 2. ÙØ­Øµ Ø§Ù„Ø­Ø°Ù ============
            deleted_phrases = [
                "this user has deleted their account",
                "user deleted their account",
                "account has been deleted"
            ]
            
            for phrase in deleted_phrases:
                if phrase in full_text:
                    return "ğŸ—‘ï¸ Ù…Ø­Ø°ÙˆÙ", url
            
            # ============ 3. ÙØ­Øµ Ø§Ù„Ù†Ø´Ø§Ø· (Ù…Ø¹Ø§ÙŠÙŠØ± ØµØ§Ø±Ù…Ø©) ============
            
            # Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ø­ÙƒÙ… Ø¨Ø§Ù„Ù†Ø´Ø§Ø·
            activity_score = 0
            
            # Ø£) Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ Ø§Ù„Ù‚ÙˆÙŠØ© (+3 Ù†Ù‚Ø§Ø·)
            strong_profile_elements = [
                soup.find('div', {'data-testid': 'user-profile'}),
                soup.select('div[data-testid*="profile"]'),
                soup.find('main', {'role': 'main'})  # Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
            ]
            
            if any(element for element in strong_profile_elements if element):
                activity_score += 3
            
            # Ø¨) Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (+2 Ù†Ù‚Ø§Ø·)
            user_content_elements = [
                soup.select('div[data-testid*="post"]'),
                soup.select('div[data-testid*="comment"]'),
                soup.select('article')
            ]
            
            if any(element for element in user_content_elements if element):
                activity_score += 2
            
            # Ø¬) ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù‚ÙˆÙŠØ© (+1 Ù†Ù‚Ø·Ø© Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø©)
            strong_keywords = [
                "post karma", "comment karma", "awardee karma",
                "cake day", "joined reddit", "reddit premium"
            ]
            
            for keyword in strong_keywords:
                if keyword in full_text:
                    activity_score += 1
            
            # Ø¯) ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…ØªÙˆØ³Ø·Ø© (+0.5 Ù†Ù‚Ø·Ø© Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø©)
            medium_keywords = [
                "trophy case", "overview", "posts", "comments",
                "about", "achievements", "submitted"
            ]
            
            medium_score = sum(0.5 for keyword in medium_keywords if keyword in full_text)
            activity_score += medium_score
            
            # Ù‡Ù€) Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙ†Ù‚Ù„ (+1 Ù†Ù‚Ø·Ø©)
            navigation_elements = [
                soup.find('nav'),
                soup.select('a[href*="posts"]'),
                soup.select('a[href*="comments"]'),
                soup.select('a[href*="overview"]')
            ]
            
            if any(element for element in navigation_elements if element):
                activity_score += 1
            
            # Ùˆ) Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ (+1 Ù†Ù‚Ø·Ø©)
            interaction_elements = [
                soup.find('button'),
                soup.select('[class*="vote"]'),
                soup.select('[class*="karma"]')
            ]
            
            if any(element for element in interaction_elements if element):
                activity_score += 1
            
            # ============ 4. Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· ============
            
            # Ù†Ø´Ø·: ÙŠØ­ØªØ§Ø¬ 4+ Ù†Ù‚Ø§Ø·
            if activity_score >= 4:
                return "âœ… Ù†Ø´Ø·", url
            
            # Ù†Ø´Ø§Ø· Ù…Ø­Ø¯ÙˆØ¯: 2-3 Ù†Ù‚Ø§Ø·
            elif activity_score >= 2:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¹Ù„Ø§Ù…Ø§Øª Ø®Ø·Ø£
                error_signs = [
                    "not found", "doesn't exist", "unavailable",
                    "removed", "no longer exists"
                ]
                has_errors = any(error in full_text for error in error_signs)
                
                if not has_errors:
                    return "âœ… Ù†Ø´Ø·", url
                else:
                    return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            # Ù†Ù‚Ø§Ø· Ù‚Ù„ÙŠÙ„Ø©: ÙØ­Øµ Ø¥Ø¶Ø§ÙÙŠ
            elif activity_score >= 1:
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø­ØªÙˆÙ‰ Reddit ÙƒØ§ÙÙŠ Ø¨Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡
                if "reddit" in full_text and len(full_text) > 300:
                    error_signs = ["not found", "doesn't exist", "error"]
                    has_errors = any(error in full_text for error in error_signs)
                    
                    if not has_errors and "user" in full_text:
                        return "â“ Ø­Ø§Ù„Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©", url
                    else:
                        return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
                else:
                    return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
            
            # Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù‚Ø§Ø· ÙƒØ§ÙÙŠØ©
            else:
                return "âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", url
                    
    except httpx.TimeoutException:
        return "â±ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„", url
    except httpx.ConnectError:
        return "ğŸŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª", url
    except Exception as e:
        return f"âš ï¸ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)[:50]}...", url

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
col1, col2 = st.columns([3, 1])

with col1:
    user_input = st.text_input(
        "Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨:",
        placeholder="Ù…Ø«Ø§Ù„: username Ø£Ùˆ u/username Ø£Ùˆ https://reddit.com/u/username",
        help="ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø£ÙŠ ØµÙŠØºØ©"
    )

with col2:
    st.markdown("<br>", unsafe_allow_html=True)  # Ù…Ø³Ø§Ø­Ø© ÙØ§Ø±ØºØ© Ù„Ù„Ù…Ø­Ø§Ø°Ø§Ø©
    check_button = st.button("ğŸ” ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨", type="primary")

if check_button and user_input.strip():
    username = clean_username(user_input)
    
    if not username:
        st.error("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… ØµØ§Ù„Ø­")
    else:
        # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙØ­Øµ
        st.info(f"ğŸ” Ø¬Ø§Ø±Ù ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨: **{username}**")
        
        # Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i in range(100):
            progress_bar.progress(i + 1)
            if i < 30:
                status_text.text("Ø¬Ø§Ø±Ù Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Reddit...")
            elif i < 70:
                status_text.text("Ø¬Ø§Ø±Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            else:
                status_text.text("Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")
            time.sleep(0.01)
        
        # ÙØ­Øµ Ø§Ù„Ø­Ø³Ø§Ø¨
        status, url = check_reddit_status(username)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar.empty()
        status_text.empty()
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¹ ØªØµÙ…ÙŠÙ… Ù…Ù…ÙŠØ²
        st.markdown("---")
        st.subheader("ğŸ“Š Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ:")
        
        if status.startswith("âœ…"):
            st.success(f"**{status}**")
            st.balloons()  # ØªØ£Ø«ÙŠØ± Ø¨ØµØ±ÙŠ Ù„Ù„Ù†Ø¬Ø§Ø­
            if url:
                st.markdown(f"ğŸ”— [Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø­Ø³Ø§Ø¨]({url})")
        elif status.startswith("ğŸš«"):
            st.error(f"**{status}**")
        elif status.startswith("âŒ") or status.startswith("ğŸ—‘ï¸"):
            st.warning(f"**{status}**")
        else:
            st.info(f"**{status}**")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        with st.expander("ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙØ­Øµ"):
            st.write(f"**Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {username}")
            if url:
                st.write(f"**Ø§Ù„Ø±Ø§Ø¨Ø·:** {url}")
            st.write(f"**ÙˆÙ‚Øª Ø§Ù„ÙØ­Øµ:** {time.strftime('%Y-%m-%d %H:%M:%S')}")

elif check_button:
    st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø³Ø§Ø¨ Ø£ÙˆÙ„Ø§Ù‹")



# ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>ğŸ› ï¸ ØªÙ… ØªØ·ÙˆÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ÙØ­Øµ Ø­Ø³Ø§Ø¨Ø§Øª Reddit Ø¨Ø³Ù‡ÙˆÙ„Ø© ÙˆØ³Ø±Ø¹Ø©</p>
    <p>ğŸ’» Ù…Ø·ÙˆØ± Ø¨ØªÙ‚Ù†ÙŠØ© Streamlit | ğŸ”’ Ø¢Ù…Ù† ÙˆØ³Ø±ÙŠØ¹</p>
</div>
""", unsafe_allow_html=True)
