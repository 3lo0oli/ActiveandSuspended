import streamlit as st
import httpx
import re
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import json

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Reddit",
    page_icon="ğŸ¤–",
    layout="centered"
)

# CSS Ù…Ø®ØµØµ
st.markdown("""
<style>
    .header {
        text-align: center;
        color: #FF4500;
        margin-bottom: 30px;
    }
    .result-box {
        padding: 20px;
        border-radius: 8px;
        margin: 20px 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    .active { border-left: 5px solid #4CAF50; background-color: #e8f5e9; }
    .suspended { border-left: 5px solid #f44336; background-color: #ffebee; }
    .not-found { border-left: 5px solid #FF9800; background-color: #fff3e0; }
    .unknown { border-left: 5px solid #9e9e9e; background-color: #f5f5f5; }
    .stButton>button {
        background-color: #FF4500;
        color: white;
        font-weight: bold;
        border: none;
        padding: 0.5rem 1rem;
        border-radius: 0.5rem;
        width: 100%;
    }
    .debug-info {
        font-family: monospace;
        font-size: 0.8rem;
        background: #f5f5f5;
        padding: 10px;
        border-radius: 4px;
        margin-top: 10px;
    }
</style>
""", unsafe_allow_html=True)

# Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.markdown("<h1 class='header'>ğŸ¤– Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Reddit</h1>", unsafe_allow_html=True)

# Ø¯Ø§Ù„Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
def extract_username(input_url):
    try:
        if not input_url:
            return None
            
        input_url = input_url.strip().strip("/")
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø¨Ø§Ø´Ø± Ø¨Ø¯ÙˆÙ† Ø±Ø§Ø¨Ø·
        if not any(x in input_url for x in ['http://', 'https://', 'reddit.com']):
            return re.sub(r'[^a-zA-Z0-9_-]', '', input_url.split('?')[0].split('/')[0])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        parsed = urlparse(input_url)
        if not parsed.scheme:
            parsed = urlparse(f"https://{input_url}")
        
        path_parts = [p for p in parsed.path.split('/') if p]
        
        if 'user' in path_parts:
            return path_parts[path_parts.index('user') + 1]
        elif 'u' in path_parts:
            return path_parts[path_parts.index('u') + 1]
        
        return path_parts[0] if path_parts else None
    
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {str(e)}")
        return None

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
def advanced_reddit_check(username):
    if not username:
        return "âŒ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± ØµØ­ÙŠØ­", "unknown", None, {}
    
    url = f"https://www.reddit.com/user/{username}/"
    api_url = f"https://www.reddit.com/user/{username}/about.json"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9"
    }
    
    debug_info = {"username": username, "checks": []}
    
    try:
        with httpx.Client(follow_redirects=True, timeout=15) as client:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API) Ø£ÙˆÙ„Ø§Ù‹
            api_response = client.get(api_url, headers=headers)
            debug_info["api_status"] = api_response.status_code
            
            if api_response.status_code == 200:
                try:
                    data = api_response.json()
                    if 'error' in data and data['error'] == 404:
                        debug_info["checks"].append("API: Account not found")
                        return "âŒ Ø§Ù„Ø­Ø³Ø§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", "not-found", url, debug_info
                    if 'data' in data and 'is_suspended' in data['data']:
                        if data['data']['is_suspended']:
                            debug_info["checks"].append("API: Account suspended")
                            return "ğŸ”´ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…ÙˆÙ‚ÙˆÙ", "suspended", url, debug_info
                        else:
                            debug_info["checks"].append("API: Account active")
                            return "ğŸŸ¢ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù†Ø´Ø·", "active", url, debug_info
                except json.JSONDecodeError:
                    pass
            
            # Ø¥Ø°Ø§ ÙØ´Ù„ API Ù†Ù„Ø¬Ø£ Ù„ÙØ­Øµ Ø§Ù„ØµÙØ­Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
            response = client.get(url, headers=headers)
            debug_info["page_status"] = response.status_code
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø¯Ù‚Ø©
            page_text = soup.get_text().lower()
            debug_info["page_text_snippet"] = page_text[:200] + "..." if page_text else ""
            
            # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„ÙƒÙ„ Ø­Ø§Ù„Ø©
            suspended_keywords = [
                "this account has been suspended",
                "account suspended",
                "suspended account",
                "content unavailable",
                "user suspended",
                "Ù‡Ø°Ø§ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…ÙˆÙ‚ÙˆÙ"
            ]
            
            not_found_keywords = [
                "page not found",
                "sorry, nobody on reddit goes by that name",
                "there's nobody on reddit by that name",
                "user not found",
                "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³Ù…"
            ]
            
            active_indicators = [
                f"u/{username}",
                f"user/{username}",
                "post karma",
                "comment karma",
                "cake day",
                "Ù…Ù†Ø´ÙˆØ±Ø§Øª",
                "ØªØ¹Ù„ÙŠÙ‚Ø§Øª"
            ]
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆÙ‚ÙˆÙ
            if any(kw in page_text for kw in suspended_keywords):
                debug_info["checks"].append("Page: Suspended keywords found")
                return "ğŸ”´ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù…ÙˆÙ‚ÙˆÙ", "suspended", url, debug_info
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
            if response.status_code == 404 or any(kw in page_text for kw in not_found_keywords):
                debug_info["checks"].append("Page: Not found indicators")
                return "âŒ Ø§Ù„Ø­Ø³Ø§Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", "not-found", url, debug_info
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø´Ø·
            if response.status_code == 200:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± ÙˆØµÙÙŠØ© Ù…Ø­Ø¯Ø¯Ø©
                profile_header = soup.find("div", {"class": "profile-header"})
                profile_tabs = soup.find("div", {"class": "profile-tabs"})
                
                if any(ind in page_text for ind in active_indicators) or (profile_header and profile_tabs):
                    debug_info["checks"].append("Page: Active indicators found")
                    return "ğŸŸ¢ Ø§Ù„Ø­Ø³Ø§Ø¨ Ù†Ø´Ø·", "active", url, debug_info
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©
            debug_info["checks"].append("Page: No clear indicators")
            return "âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø©", "unknown", url, debug_info
    
    except httpx.TimeoutException:
        debug_info["error"] = "Request timeout"
        return "âš ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨", "unknown", url, debug_info
    except Exception as e:
        debug_info["error"] = str(e)
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}", "unknown", url, debug_info

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
input_url = st.text_input(
    "Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø­Ø³Ø§Ø¨ Reddit Ø£Ùˆ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:",
    placeholder="Ù…Ø«Ø§Ù„: nedaa_7 Ø£Ùˆ https://www.reddit.com/user/nedaa_7/",
    key="user_input"
)

check_button = st.button("ØªØ­Ù‚Ù‚ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§")

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
if check_button:
    if input_url:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©..."):
            username = extract_username(input_url)
            
            if username:
                status, status_class, profile_url, debug_info = advanced_reddit_check(username)
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                st.markdown(
                    f"""
                    <div class="result-box {status_class}">
                        <h3>{status}</h3>
                        <p><strong>Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:</strong> {username}</p>
                        <p><strong>Ø§Ù„Ø±Ø§Ø¨Ø·:</strong> {profile_url}</p>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
                
                # Ø¥Ø¸Ù‡Ø§Ø± ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©
                with st.expander("ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ­Ù‚Ù‚", expanded=False):
                    st.write("**Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚:**")
                    if "API" in str(debug_info.get("checks", [])):
                        st.success("ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API) Ù„Ù„ØªØ­Ù‚Ù‚")
                    else:
                        st.info("ØªÙ… ÙØ­Øµ Ø§Ù„ØµÙØ­Ø© Ù…Ø¨Ø§Ø´Ø±Ø©")
                    
                    st.write("**Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©:**")
                    st.json(debug_info)
            else:
                st.error("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… ØµØ­ÙŠØ­")
    else:
        st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø· Ø£Ùˆ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… Ø£ÙˆÙ„Ø§Ù‹")

# ØªØ°ÙŠÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
st.markdown("---")
st.markdown("""
<p style="text-align: center; color: #666; font-size: 0.9rem;">
    Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© | Ù„Ø§ ØªØ­ØªØ§Ø¬ Ù„Ø£ÙŠ ØªØ¯Ø®Ù„ ÙŠØ¯ÙˆÙŠ
</p>
""", unsafe_allow_html=True)
